---
phase: 33-scoring-configuration-backend
plan: 03
type: execute
wave: 2
depends_on: ["33-01"]
files_modified:
  - job_radar/wizard.py
  - tests/test_wizard.py
autonomous: true

must_haves:
  truths:
    - "New profiles created by the wizard include scoring_weights with default values"
    - "New profiles created by the wizard include staffing_preference field"
    - "Users can choose to customize scoring weights or use defaults during wizard setup"
    - "Staffing preference question offers three choices: Neutral, Boost, Penalize"
    - "Schema version 2 is set on new profiles created by the wizard"
  artifacts:
    - path: "job_radar/wizard.py"
      provides: "Scoring weight and staffing preference questions in setup wizard"
      contains: "scoring_weights"
    - path: "tests/test_wizard.py"
      provides: "Tests for wizard scoring weight and staffing preference integration"
      contains: "test_wizard_profile_has_scoring_weights"
  key_links:
    - from: "job_radar/wizard.py"
      to: "job_radar/profile_manager.py"
      via: "imports DEFAULT_SCORING_WEIGHTS for new profile defaults"
      pattern: "from .profile_manager import.*DEFAULT_SCORING_WEIGHTS"
    - from: "job_radar/wizard.py"
      to: "save_profile"
      via: "profile_data includes scoring_weights and staffing_preference before save"
      pattern: "profile_data.*scoring_weights"
---

<objective>
Add scoring weight and staffing preference questions to the CLI setup wizard for new profile creation.

Purpose: Ensure new profiles created via the wizard include v2 schema fields (scoring_weights, staffing_preference) so the scoring engine can use them immediately. Terminal users need access to these settings, not just GUI users (per user decision).

Output: Updated wizard.py with optional scoring weight customization question and staffing preference selection, plus tests verifying wizard output includes v2 fields.
</objective>

<execution_context>
@/home/corye/.claude/get-shit-done/workflows/execute-plan.md
@/home/corye/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/33-scoring-configuration-backend/33-RESEARCH.md
@.planning/phases/33-scoring-configuration-backend/33-01-SUMMARY.md
@job_radar/wizard.py
@job_radar/profile_manager.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add scoring weight and staffing preference questions to wizard</name>
  <files>job_radar/wizard.py</files>
  <action>
Update `job_radar/wizard.py` to include v2 schema fields for new profiles:

**1. Import DEFAULT_SCORING_WEIGHTS:**
Add to the existing imports at the top:
```python
from .profile_manager import save_profile, DEFAULT_SCORING_WEIGHTS
```
(Update the existing `from .profile_manager import save_profile` line to also import DEFAULT_SCORING_WEIGHTS.)

**2. Add staffing preference question to the questions list:**
After the `dealbreakers` question (index 8) and before `min_score` (index 9), add:

```python
{
    'key': 'staffing_preference',
    'type': 'select',
    'message': "How should staffing/recruiting firms be scored?",
    'instruction': None,
    'choices': [
        "Neutral (treat same as direct employers)",
        "Boost (prefer staffing firms -- higher placement incentive)",
        "Penalize (avoid staffing firms -- prefer direct employers)",
    ],
    'validator': None,
    'required': True,
    'default': "Neutral (treat same as direct employers)",
},
```

**3. Add scoring weights customization question:**
After the staffing_preference question, add:

```python
{
    'key': 'customize_weights',
    'type': 'confirm',
    'message': "Customize scoring component weights?",
    'instruction': "Advanced: adjust how much skills, title, seniority, location, domain, and response likelihood affect job scores. Most users should skip this.",
    'validator': None,
    'required': True,
    'default': False,
},
```

**4. Handle the select type in the question loop:**
In the `while idx < len(questions)` loop (around line 420), add handling for `'select'` type questions:
```python
elif q['type'] == 'select':
    prompt_kwargs_select = {
        'message': q['message'],
        'choices': q['choices'],
        'style': custom_style,
    }
    if key in answers:
        prompt_kwargs_select['default'] = answers[key]
    elif q.get('default') is not None:
        prompt_kwargs_select['default'] = q['default']
    result = questionary.select(**prompt_kwargs_select).ask()
```

**5. Update the section header index:**
The "Search Preferences" section header currently triggers at `idx == 5`. Since we added 2 questions before min_score, adjust the section header index. The scoring questions should appear in a new "Scoring Preferences" section. Add:
```python
elif idx == 9:  # Adjust index based on where staffing_preference falls
    print("\nðŸŽ¯ Scoring Preferences")
    print("-" * 40 + "\n")
```
Keep the existing Search Preferences header but adjust its index to match the new question order (it should trigger for min_score which is now at a higher index).

Actually, a cleaner approach: use the key name for section headers instead of index:
```python
# Replace index-based section headers with key-based:
if key == 'name':
    print("\n Profile Information")
    print("-" * 40 + "\n")
elif key == 'staffing_preference':
    print("\n Scoring Preferences")
    print("-" * 40 + "\n")
elif key == 'min_score':
    print("\n Search Preferences")
    print("-" * 40 + "\n")
```

**6. Handle conditional weight customization:**
If user answers `customize_weights = True`, prompt for each of the 6 weights individually. After all 6 are entered, validate they sum to 1.0 (with 0.01 tolerance). If they don't sum correctly, show error and re-prompt.

If user answers `customize_weights = False` (the default), skip weight prompts and use DEFAULT_SCORING_WEIGHTS.

For the weight prompts (only shown when customize_weights is True), add them as a separate loop AFTER the main question loop completes:

```python
# After main question loop, handle custom weights
if answers.get('customize_weights'):
    custom_weights = _prompt_custom_weights()  # New helper function
    if custom_weights is None:
        # User cancelled, fall back to defaults
        profile_data["scoring_weights"] = dict(DEFAULT_SCORING_WEIGHTS)
    else:
        profile_data["scoring_weights"] = custom_weights
else:
    profile_data["scoring_weights"] = dict(DEFAULT_SCORING_WEIGHTS)
```

Create a helper function `_prompt_custom_weights()` that:
- Shows current default values
- Prompts for each weight as a float (0.05-1.0)
- After all 6, checks sum. If not 1.0 (within tolerance), shows error and offers to retry or use defaults
- Returns the weights dict or None if user cancels

**7. Build profile_data with v2 fields:**
After the existing profile_data construction (around line 507), add:

```python
# Scoring weights (v2 schema)
# Already set above based on customize_weights answer

# Staffing preference (v2 schema)
staffing_answer = answers.get('staffing_preference', 'Neutral (treat same as direct employers)')
if 'Boost' in staffing_answer:
    profile_data['staffing_preference'] = 'boost'
elif 'Penalize' in staffing_answer:
    profile_data['staffing_preference'] = 'penalize'
else:
    profile_data['staffing_preference'] = 'neutral'
```

**8. Update the celebration summary to show scoring info:**
In the summary display (around line 520), add after dealbreakers display:

```python
# Scoring weights display
weights = profile_data.get('scoring_weights', {})
if weights == DEFAULT_SCORING_WEIGHTS:
    print(f"   Scoring Weights: Default (balanced)")
else:
    print(f"   Scoring Weights: Custom")
    for component, weight in weights.items():
        print(f"      {component}: {weight:.0%}")

staffing = profile_data.get('staffing_preference', 'neutral')
print(f"   Staffing Firms: {staffing.capitalize()}")
```

**9. Add "Scoring Weights" and "Staffing Firms" to the edit field choices:**
In the post-summary edit loop, add options to edit scoring weights and staffing preference. For scoring weights, offer a reset-to-defaults option or re-run the customization flow. For staffing preference, re-show the select prompt.
  </action>
  <verify>Run `pytest` to ensure no import errors or regressions. Manually test (if feasible): `python -c "from job_radar.wizard import run_setup_wizard"` should import without error.</verify>
  <done>Wizard asks staffing preference question with 3 choices (Neutral/Boost/Penalize). Wizard offers optional weight customization. New profiles include scoring_weights and staffing_preference fields. Summary displays scoring configuration. Edit loop supports modifying scoring settings.</done>
</task>

<task type="auto">
  <name>Task 2: Add tests for wizard v2 schema field output</name>
  <files>tests/test_wizard.py</files>
  <action>
Create or update `tests/test_wizard.py` with tests verifying wizard output includes v2 fields.

Since the wizard is interactive (uses questionary), tests should mock the questionary prompts. Use `unittest.mock.patch` to mock `questionary.text`, `questionary.confirm`, and `questionary.select`.

**Tests to add:**

1. `test_wizard_profile_has_scoring_weights` - Mock all wizard prompts with valid answers (including staffing_preference="Neutral..." and customize_weights=False). Mock `save_profile` to capture the profile_data dict. Assert profile_data contains `scoring_weights` key matching DEFAULT_SCORING_WEIGHTS.

2. `test_wizard_profile_has_staffing_preference_neutral` - Same setup as above with staffing choice "Neutral...". Assert profile_data["staffing_preference"] == "neutral".

3. `test_wizard_profile_has_staffing_preference_boost` - Mock staffing choice as "Boost...". Assert profile_data["staffing_preference"] == "boost".

4. `test_wizard_profile_has_staffing_preference_penalize` - Mock staffing choice as "Penalize...". Assert profile_data["staffing_preference"] == "penalize".

5. `test_wizard_default_weights_used_when_not_customized` - Mock customize_weights=False. Assert the saved profile_data["scoring_weights"] exactly matches DEFAULT_SCORING_WEIGHTS.

**Note on mocking approach:** The wizard uses a sequential prompt loop. The simplest approach is to mock `questionary.text().ask()`, `questionary.confirm().ask()`, and `questionary.select().ask()` to return predetermined answers in sequence using `side_effect` lists. Also mock `save_profile` and `_write_json` to capture output without filesystem writes. Mock `get_data_dir` to return a tmp_path.

If a test_wizard.py already exists, add these tests to it. If not, create it with appropriate imports.

Run `pytest tests/test_wizard.py -v` -- all tests pass.
Run `pytest` -- full suite passes.

Commit: `feat(33-03): add scoring weights and staffing preference to setup wizard`
  </action>
  <verify>Run `pytest tests/test_wizard.py -v` -- all wizard tests pass. Run `pytest` -- full test suite passes (all tests, zero regressions).</verify>
  <done>Wizard tests verify that new profiles include scoring_weights (default values) and staffing_preference. All 3 staffing preference choices are tested. Full test suite passes.</done>
</task>

</tasks>

<verification>
1. `pytest tests/test_wizard.py -v` -- all wizard tests pass
2. `pytest` -- full test suite passes (all tests, zero regressions)
3. `python -c "from job_radar.wizard import run_setup_wizard"` -- imports cleanly (no circular import or missing dependency)
4. `python -c "from job_radar.profile_manager import DEFAULT_SCORING_WEIGHTS; print(DEFAULT_SCORING_WEIGHTS)"` -- constant is importable
</verification>

<success_criteria>
- Wizard asks staffing preference question with Neutral/Boost/Penalize choices
- Wizard offers optional scoring weight customization (defaults to "No")
- New profiles include scoring_weights with DEFAULT_SCORING_WEIGHTS values
- New profiles include staffing_preference based on user selection
- Summary shows scoring configuration
- Edit loop supports modifying scoring settings
- All existing + new tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/33-scoring-configuration-backend/33-03-SUMMARY.md`
</output>
