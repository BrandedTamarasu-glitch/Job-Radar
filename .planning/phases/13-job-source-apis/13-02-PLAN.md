---
phase: 13-job-source-apis
plan: 02
type: execute
wave: 2
depends_on: ["13-01"]
files_modified:
  - job_radar/sources.py
  - job_radar/search.py
autonomous: true

must_haves:
  truths:
    - "Adzuna jobs appear in search results with title, company, location, URL, and salary data"
    - "Authentic Jobs listings appear in search results with design/creative roles"
    - "API failures (network, 500, timeout) skip silently — logged for debugging only"
    - "Rate limit hits show notice: 'Skipped {source}: rate limited, retry after {time}'"
    - "Missing API credentials skip source gracefully (no crash)"
    - "Jobs with missing required fields (title/company/url) are skipped and logged"
    - "Scrapers run first, then APIs supplement results sequentially"
    - "Cross-source deduplication runs after all sources complete"
    - "Single progress bar shows all sources"
  artifacts:
    - path: "job_radar/sources.py"
      provides: "fetch_adzuna, fetch_authenticjobs, map_adzuna_to_job_result, map_authenticjobs_to_job_result functions; updated fetch_all with sequential scraper-then-API flow"
      exports: ["fetch_adzuna", "fetch_authenticjobs"]
    - path: "job_radar/search.py"
      provides: "Updated main() pipeline with total failure messaging"
  key_links:
    - from: "job_radar/sources.py:fetch_adzuna"
      to: "job_radar/api_config.py:get_api_key"
      via: "credential check"
      pattern: "get_api_key.*ADZUNA"
    - from: "job_radar/sources.py:fetch_adzuna"
      to: "job_radar/rate_limits.py:check_rate_limit"
      via: "rate limit check"
      pattern: "check_rate_limit.*adzuna"
    - from: "job_radar/sources.py:fetch_all"
      to: "job_radar/deduplication.py:deduplicate_cross_source"
      via: "post-fetch deduplication"
      pattern: "deduplicate_cross_source"
    - from: "job_radar/sources.py:map_adzuna_to_job_result"
      to: "job_radar/sources.py:strip_html_and_normalize"
      via: "description cleaning"
      pattern: "strip_html_and_normalize"
    - from: "job_radar/sources.py:map_adzuna_to_job_result"
      to: "job_radar/sources.py:parse_location_to_city_state"
      via: "location normalization"
      pattern: "parse_location_to_city_state"
---

<objective>
Implement Adzuna and Authentic Jobs API fetchers with per-source mapper functions, integrate into the fetch pipeline with sequential scraper-then-API flow, and add cross-source deduplication.

Purpose: This is the core deliverable of Phase 13 — users get job listings from two new API sources integrated into their existing search results, with proper error handling and deduplication.
Output: Two new API fetcher functions, two mapper functions, updated fetch_all() with sequential flow and deduplication, updated build_search_queries() to include API sources.
</objective>

<execution_context>
@/Users/coryebert/.claude/get-shit-done/workflows/execute-plan.md
@/Users/coryebert/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/13-job-source-apis/13-CONTEXT.md
@.planning/phases/13-job-source-apis/13-RESEARCH.md
@.planning/phases/13-job-source-apis/13-01-SUMMARY.md
@.planning/phases/12-api-foundation/12-01-SUMMARY.md
@.planning/phases/12-api-foundation/12-02-SUMMARY.md
@job_radar/sources.py
@job_radar/search.py
@job_radar/api_config.py
@job_radar/rate_limits.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement Adzuna and Authentic Jobs fetchers with mapper functions</name>
  <files>job_radar/sources.py</files>
  <action>
  Add two new API fetcher sections to job_radar/sources.py (after the We Work Remotely section, before the manual-check URL generators section).

  **Imports to add at top of file:**
  ```python
  import html  # (if not already added by Plan 01)
  from .api_config import get_api_key
  from .rate_limits import check_rate_limit
  ```

  **1. Adzuna API fetcher section:**

  Add section divider:
  ```python
  # ---------------------------------------------------------------------------
  # Adzuna API fetcher
  # ---------------------------------------------------------------------------
  ```

  `fetch_adzuna(query: str, location: str = "", verbose: bool = False) -> list[JobResult]`:
  Per CONTEXT.md decisions for error handling:
  - Check credentials first: `get_api_key("ADZUNA_APP_ID", "Adzuna")` and `get_api_key("ADZUNA_APP_KEY", "Adzuna")`. If either missing, return [] (get_api_key logs warning).
  - Check rate limit: `check_rate_limit("adzuna", verbose=verbose)`. If False, return [] (check_rate_limit logs notice with retry time).
  - Build API URL: `https://api.adzuna.com/v1/api/jobs/us/search/1` with params: app_id, app_key, what=query, results_per_page=50, where=location (if provided).
  - Use `fetch_with_retry(url, headers=HEADERS, use_cache=True)` for the HTTP call.
  - Parse JSON response: `data.get("results", [])` for job items.
  - Map each item with `map_adzuna_to_job_result(item)`. Skip None results (invalid jobs).
  - Error handling per CONTEXT.md:
    - requests.exceptions.HTTPError: 401/403 -> log.error with setup-apis suggestion. Other -> log.debug (silent skip).
    - json.JSONDecodeError -> log.debug (silent skip)
    - Exception -> log.debug (silent skip, broad catch for crash tolerance)
  - Import `requests` only where needed for the HTTPError check (use `import requests` in the try block, or catch the generic form).
  - Return list of JobResult.

  `map_adzuna_to_job_result(item: dict) -> JobResult | None`:
  Per CONTEXT.md strict validation — skip if missing title/company/url:
  - title: `item.get("title", "").strip()`
  - company: `item.get("company", {}).get("display_name", "").strip()`
  - url: `item.get("redirect_url", "").strip()`
  - If any empty, log.debug and return None.
  - Location: `item.get("location", {}).get("display_name", "")` -> `parse_location_to_city_state()`
  - Salary: `item.get("salary_min")`, `item.get("salary_max")`, currency="USD" (Adzuna US endpoint). Format salary string for backward compat: "$X - $Y", "$X+", or "Not specified".
  - Description: `item.get("description", "")` -> `strip_html_and_normalize()`, truncate to 500 chars.
  - Arrangement: `_parse_arrangement(f"{title} {description}")`
  - Employment type: Map `contract_type` and `contract_time` fields. "permanent" -> "Permanent", "full_time" -> "Full-time", combine as needed.
  - Date posted: `item.get("created", "")`
  - Return JobResult with source="adzuna", parse_confidence="high", salary_min/max/currency set.
  - Use `_clean_field()` for title, company, location.

  **2. Authentic Jobs API fetcher section:**

  Add section divider:
  ```python
  # ---------------------------------------------------------------------------
  # Authentic Jobs API fetcher
  # ---------------------------------------------------------------------------
  ```

  `fetch_authenticjobs(query: str, location: str = "", verbose: bool = False) -> list[JobResult]`:
  Same error handling pattern as Adzuna:
  - Check credentials: `get_api_key("AUTHENTIC_JOBS_API_KEY", "Authentic Jobs")`. Missing -> return [].
  - Check rate limit: `check_rate_limit("authentic_jobs", verbose=verbose)`. False -> return [].
  - Build API URL: `https://authenticjobs.com/api/` with params: api_key, method="aj.jobs.search", format="json", keywords=query, location=location (if provided), perpage=50.
  - Use `fetch_with_retry(url, headers=HEADERS, use_cache=True)`.
  - Parse JSON: Response has `listings.listing` array (may be dict if single result — handle both).
  - Map with `map_authenticjobs_to_job_result(item)`. Skip None.
  - Same error handling pattern as Adzuna.

  `map_authenticjobs_to_job_result(item: dict) -> JobResult | None`:
  Strict validation — skip if missing required fields:
  - title: `item.get("title", "").strip()`
  - company: `item.get("company", {}).get("name", "").strip()` (Authentic Jobs nests company info)
  - url: `item.get("url", "").strip()` or `item.get("apply_url", "").strip()`
  - If any empty, log.debug and return None.
  - Location: `item.get("company", {}).get("location", {}).get("name", "")` -> `parse_location_to_city_state()`
  - Salary: Not typically available from Authentic Jobs -> salary="Not specified", salary_min/max/currency=None.
  - Description: `item.get("description", "")` -> `strip_html_and_normalize()`, truncate to 500 chars.
  - Arrangement: `_parse_arrangement(f"{title} {description}")`
  - Employment type: `item.get("type", {}).get("name", "")` (Authentic Jobs provides job type)
  - Date posted: `item.get("post_date", "")`
  - Return JobResult with source="authentic_jobs", parse_confidence="high".
  - Use `_clean_field()` for title, company, location.

  NOTE on Authentic Jobs: The API docs are incomplete (see RESEARCH.md Open Questions #1). The mapper should be defensive — if field structure differs from expected, gracefully handle with .get() chains and empty string defaults. The mapper can be refined once live API testing is done.
  </action>
  <verify>
  Run: `python -c "from job_radar.sources import fetch_adzuna, fetch_authenticjobs, map_adzuna_to_job_result, map_authenticjobs_to_job_result; print('All fetcher functions import OK')"`
  Test mapper with mock data:
  `python -c "
from job_radar.sources import map_adzuna_to_job_result
job = map_adzuna_to_job_result({'title': 'Engineer', 'company': {'display_name': 'Acme'}, 'redirect_url': 'http://x', 'location': {'display_name': 'Remote'}, 'salary_min': 80000, 'salary_max': 120000, 'description': '<p>Great job</p>', 'created': '2026-02-10'})
print(f'Title: {job.title}, Company: {job.company}, Salary: {job.salary}, Location: {job.location}, salary_min: {job.salary_min}')
"`
  Expected: Title: Engineer, Company: Acme, Salary: $80,000 - $120,000, Location: Remote, salary_min: 80000
  Test invalid job skipping:
  `python -c "from job_radar.sources import map_adzuna_to_job_result; assert map_adzuna_to_job_result({'title': '', 'company': {}, 'redirect_url': ''}) is None; print('Invalid job correctly skipped')"`
  </verify>
  <done>
  fetch_adzuna() and fetch_authenticjobs() implemented with proper credential checking, rate limiting, error handling (silent skip for network/500, notice for rate limit, error for 401/403). Mapper functions validate required fields, strip HTML from descriptions, normalize locations, and populate salary fields. Invalid jobs are skipped and logged.
  </done>
</task>

<task type="auto">
  <name>Task 2: Integrate API sources into fetch pipeline with sequential flow and deduplication</name>
  <files>job_radar/sources.py, job_radar/search.py</files>
  <action>
  **1. Update build_search_queries() in sources.py:**

  Add API source queries after existing scraper queries:

  ```python
  # Adzuna queries: each target title (same pattern as Dice)
  for title in titles:
      queries.append({
          "source": "adzuna",
          "query": title,
          "location": location,
      })

  # Authentic Jobs queries: top 2 target titles
  for title in titles[:2]:
      queries.append({
          "source": "authentic_jobs",
          "query": title,
          "location": location,
      })
  ```

  **2. Modify fetch_all() in sources.py for sequential scraper-then-API flow:**

  Per CONTEXT.md decisions:
  - Scrapers first, then APIs (sequential)
  - Single progress bar for all sources
  - Wait for all sources before showing results
  - Cross-source deduplication after all results collected

  Add import at top: `from .deduplication import deduplicate_cross_source`

  Modify fetch_all() to split scraper and API queries:

  ```python
  SCRAPER_SOURCES = {"dice", "hn_hiring", "remoteok", "weworkremotely"}
  API_SOURCES = {"adzuna", "authentic_jobs"}
  ```

  Flow:
  a. Separate queries into scraper_queries and api_queries based on source.
  b. Phase 1 — Run scraper queries in parallel (existing ThreadPoolExecutor logic).
  c. Phase 2 — Run API queries in parallel (same ThreadPoolExecutor pattern, but AFTER scrapers complete).
  d. Combine all results.
  e. Run deduplicate_cross_source() on combined results.
  f. Return deduplicated results.

  Update run_query() dispatcher inside fetch_all() to handle new sources:
  ```python
  elif q["source"] == "adzuna":
      return fetch_adzuna(q["query"], q.get("location", ""))
  elif q["source"] == "authentic_jobs":
      return fetch_authenticjobs(q["query"], q.get("location", ""))
  ```

  The progress callback system already works with _SOURCE_DISPLAY_NAMES (updated in Plan 01). Keep source tracking logic but split into two phases:

  Refactor fetch_all() to use a helper function `_run_queries_parallel(queries, seen, on_progress, on_source_progress, ...)` that contains the existing ThreadPoolExecutor+as_completed logic. Call it twice: once for scraper_queries, once for api_queries. This avoids code duplication.

  Progress tracking should be continuous across both phases. Total = len(all queries). Sources_started and sources_done track across both phases.

  **3. Update search.py for total failure messaging:**

  Per CONTEXT.md decisions, after fetch_all returns, check if results are empty:
  - If all_results is empty AND at least one source had an error: "All sources failed — check your internet connection and API credentials"
  - If all_results is empty AND no errors (legitimate empty): "All sources returned zero matches — try broadening your search terms"

  This can be done by checking len(all_results) == 0 after the fetch_all call. The distinction between "failed" vs "zero matches" can be inferred: if fetch_all catches exceptions internally and returns [], we can add an optional error_count return or check by calling fetch_all and examining the result. Simplest approach: add a second return value to fetch_all or set a module-level flag.

  Simpler approach: In search.py, if all_results is empty, print a helpful message. The exact cause distinction (network failure vs zero matches) is already partially handled by logging. For user-facing message:
  - Print "No results from any source" with suggestion to check connection and API keys.
  - Don't overcomplicate — the existing "No matches found" message in search.py at line 640 already handles post-scoring empty. Add pre-filter empty check.
  </action>
  <verify>
  Run dry-run to verify queries include new sources:
  `python -c "
from job_radar.sources import build_search_queries
profile = {'name': 'Test', 'target_titles': ['Software Engineer', 'Backend Dev'], 'core_skills': ['python', 'django'], 'target_market': 'San Francisco, CA'}
queries = build_search_queries(profile)
sources = set(q['source'] for q in queries)
print(f'Total queries: {len(queries)}')
print(f'Sources: {sorted(sources)}')
assert 'adzuna' in sources, 'Adzuna not in sources'
assert 'authentic_jobs' in sources, 'Authentic Jobs not in sources'
print('All API sources included in queries')
"`
  Expected: Sources include adzuna and authentic_jobs.

  Run import check:
  `python -c "from job_radar.sources import fetch_all; print('fetch_all imports OK (includes deduplication)')"`

  Run existing tests: `python -m pytest tests/ -x -q`
  </verify>
  <done>
  Adzuna and Authentic Jobs queries added to build_search_queries(). fetch_all() runs scrapers first, then APIs, with cross-source deduplication after all results collected. Progress callbacks work across both phases. run_query() dispatches to new fetcher functions. Empty result messaging improved.
  </done>
</task>

</tasks>

<verification>
1. `python -c "from job_radar.sources import fetch_adzuna, fetch_authenticjobs; print('Fetchers import OK')"` — No import errors
2. `python -c "from job_radar.sources import build_search_queries; q = build_search_queries({'name': 'T', 'target_titles': ['Dev'], 'core_skills': ['python']}); print([x['source'] for x in q])"` — Shows adzuna and authentic_jobs in list
3. Mock mapper test: Valid Adzuna item returns JobResult with salary fields populated
4. Mock mapper test: Item missing title returns None (strict validation)
5. `python -m pytest tests/ -x -q` — All existing tests pass
6. `python -c "from job_radar.sources import fetch_all; print('Pipeline integration OK')"` — deduplication import wired
</verification>

<success_criteria>
- fetch_adzuna() handles credentials, rate limits, and all error types per CONTEXT.md
- fetch_authenticjobs() follows same pattern
- Mapper functions validate required fields strictly (skip invalid)
- HTML stripped from descriptions, locations normalized
- build_search_queries() includes adzuna and authentic_jobs queries
- fetch_all() runs scrapers first, then APIs sequentially
- Cross-source deduplication runs on combined results
- Progress callbacks work across both fetch phases
- All existing tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/13-job-source-apis/13-02-SUMMARY.md`
</output>
