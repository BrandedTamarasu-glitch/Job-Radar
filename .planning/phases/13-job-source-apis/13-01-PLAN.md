---
phase: 13-job-source-apis
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - job_radar/sources.py
  - job_radar/deduplication.py
  - pyproject.toml
autonomous: true

must_haves:
  truths:
    - "JobResult dataclass has optional salary_min, salary_max, salary_currency fields that default to None"
    - "Existing scraper code (Dice, HN Hiring, RemoteOK, WWR) still works without providing new fields"
    - "strip_html_and_normalize() removes HTML tags, decodes entities, collapses whitespace"
    - "parse_location_to_city_state() normalizes locations to 'City, State' format"
    - "deduplicate_cross_source() removes fuzzy duplicates using rapidfuzz token_sort_ratio"
    - "rapidfuzz is listed as a dependency in pyproject.toml"
  artifacts:
    - path: "job_radar/sources.py"
      provides: "Extended JobResult with salary fields, text cleaning utilities, location parsing"
      contains: "salary_min"
    - path: "job_radar/deduplication.py"
      provides: "Cross-source fuzzy deduplication module"
      exports: ["deduplicate_cross_source"]
    - path: "pyproject.toml"
      provides: "rapidfuzz dependency"
      contains: "rapidfuzz"
  key_links:
    - from: "job_radar/deduplication.py"
      to: "rapidfuzz.fuzz"
      via: "import"
      pattern: "from rapidfuzz import fuzz"
    - from: "job_radar/sources.py"
      to: "strip_html_and_normalize"
      via: "function definition"
      pattern: "def strip_html_and_normalize"
---

<objective>
Create the foundation modules for Phase 13 API integration: extend JobResult with optional salary fields, add text cleaning utilities (HTML stripping, location parsing), create cross-source fuzzy deduplication module, and add rapidfuzz dependency.

Purpose: All API fetchers and pipeline integration depend on these foundation pieces. Schema extension must be backward compatible. Deduplication must use rapidfuzz for cross-source fuzzy matching.
Output: Extended JobResult dataclass, utility functions in sources.py, new deduplication.py module, rapidfuzz in pyproject.toml.
</objective>

<execution_context>
@/Users/coryebert/.claude/get-shit-done/workflows/execute-plan.md
@/Users/coryebert/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/13-job-source-apis/13-CONTEXT.md
@.planning/phases/13-job-source-apis/13-RESEARCH.md
@.planning/phases/12-api-foundation/12-01-SUMMARY.md
@job_radar/sources.py
@pyproject.toml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Extend JobResult and add text utility functions</name>
  <files>job_radar/sources.py</files>
  <action>
  In job_radar/sources.py, make these changes:

  1. **Extend JobResult dataclass** — Add three optional fields at the END (after parse_confidence) for backward compatibility:
     ```python
     salary_min: float | None = None
     salary_max: float | None = None
     salary_currency: str | None = None
     ```
     Per CONTEXT.md: available for API sources, None for scraper sources. Per Python dataclass rules: new fields with defaults must come after existing defaults.

  2. **Add strip_html_and_normalize() function** — Add in a new "Text cleaning utilities" section above the fetcher sections:
     ```python
     def strip_html_and_normalize(text: str) -> str:
         """Strip HTML tags, decode entities, normalize whitespace."""
         if not text:
             return ""
         text = html.unescape(text)
         soup = BeautifulSoup(text, "html.parser")
         plain_text = soup.get_text(separator=" ")
         normalized = re.sub(r'\s+', ' ', plain_text)
         return normalized.strip()
     ```
     Add `import html` to the imports at top. Per CONTEXT.md: strip all HTML tags, decode HTML entities (&amp; -> &), normalize whitespace.

  3. **Add parse_location_to_city_state() function** — In the same text utilities section:
     - Handle empty string -> "Unknown"
     - Handle "remote" (case-insensitive) -> "Remote"
     - Pattern 1: "City, STATE_ABBR" already correct -> return as-is
     - Pattern 2: "City, State Name" -> abbreviate using STATE_ABBREV dict (include all 50 US states)
     - Pattern 3: "City, State, Country" -> extract City, State (common Adzuna format)
     - Fallback: return raw string (per CONTEXT.md: better to show raw than guess wrong)
     - Include regex pattern `r'^([^,]+),\s*([A-Z]{2})(?:\s|,|$)'` for state abbreviation detection

  4. **Update _SOURCE_DISPLAY_NAMES dict** — Add entries for new API sources:
     ```python
     "adzuna": "Adzuna",
     "authentic_jobs": "Authentic Jobs",
     ```

  Do NOT modify any existing fetcher functions. Do NOT change existing field order in JobResult.
  </action>
  <verify>
  Run: `python -c "from job_radar.sources import JobResult, strip_html_and_normalize, parse_location_to_city_state; j = JobResult(title='Test', company='Co', location='NY', arrangement='remote', salary='100k', date_posted='today', description='desc', url='http://x', source='test'); print(j.salary_min, j.salary_max, j.salary_currency); print(strip_html_and_normalize('<p>Hello &amp; world</p>')); print(parse_location_to_city_state('San Francisco, California, United States'))"`
  Expected: `None None None` then `Hello & world` then `San Francisco, CA`
  Also verify existing tests still pass: `python -m pytest tests/ -x -q`
  </verify>
  <done>
  JobResult has salary_min/salary_max/salary_currency fields (default None). strip_html_and_normalize strips tags, decodes entities, normalizes whitespace. parse_location_to_city_state handles US states, Remote, and international locations. All existing tests pass.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create deduplication module and add rapidfuzz dependency</name>
  <files>job_radar/deduplication.py, pyproject.toml</files>
  <action>
  1. **Add rapidfuzz to pyproject.toml** — Add "rapidfuzz" to the dependencies list in pyproject.toml.

  2. **Install rapidfuzz** — Run `pip install rapidfuzz` to make it available.

  3. **Create job_radar/deduplication.py** — New module with deduplicate_cross_source() function:

     Per CONTEXT.md decisions:
     - Match criteria: title + company + location similarity
     - Use rapidfuzz token_sort_ratio (word-order independent)
     - Threshold: 85 for title/company, 80 for location
     - Keep first occurrence (preserves source priority — scrapers run first)
     - Not just source+job_id (would miss cross-source duplicates)

     Implementation per RESEARCH.md Pattern 4:
     ```python
     """Cross-source fuzzy deduplication for job listings."""
     import logging
     from collections import defaultdict
     from rapidfuzz import fuzz

     log = logging.getLogger(__name__)

     def deduplicate_cross_source(results: list, threshold: int = 85) -> list:
     ```

     Optimization: Bucket by normalized company first word to reduce O(N^2) to O(N*B).

     Fast path: exact duplicate check using (title.lower(), company.lower(), location.lower()) set.

     Fuzzy path: For each job in bucket, compare against seen jobs in same bucket using:
     - `fuzz.token_sort_ratio(job.title, seen_job.title)` >= threshold
     - `fuzz.token_sort_ratio(job.company, seen_job.company)` >= threshold
     - `fuzz.ratio(job.location, seen_job.location)` >= 80
     All three must match to be considered duplicate.

     Log deduplication stats: "Deduplication: {N} -> {M} jobs ({D} duplicates removed)"

     Handle edge cases:
     - Empty results list -> return []
     - Empty company name -> bucket key "unknown"
     - Single result -> return [result] (no comparison needed)

  Follow existing codebase conventions:
  - Module docstring at top
  - `log = logging.getLogger(__name__)`
  - Type hints on function signatures
  - Docstring with behavior description
  </action>
  <verify>
  Run: `python -c "from job_radar.deduplication import deduplicate_cross_source; from job_radar.sources import JobResult; j1 = JobResult(title='Software Engineer', company='Google Inc', location='Remote', arrangement='remote', salary='150k', date_posted='today', description='desc', url='http://a', source='Dice'); j2 = JobResult(title='Software Engineer', company='Google Inc.', location='Remote', arrangement='remote', salary='150k', date_posted='today', description='desc', url='http://b', source='adzuna'); result = deduplicate_cross_source([j1, j2]); print(f'Input: 2, Output: {len(result)}, Kept: {result[0].source}')"`
  Expected: `Input: 2, Output: 1, Kept: Dice` (first occurrence kept)
  Also run: `pip show rapidfuzz` to confirm installation.
  </verify>
  <done>
  deduplication.py module created with deduplicate_cross_source() using rapidfuzz. Fuzzy matching detects cross-source duplicates by title+company+location similarity. rapidfuzz added to pyproject.toml dependencies. Bucketing optimization reduces comparison count.
  </done>
</task>

</tasks>

<verification>
1. `python -c "from job_radar.sources import JobResult; j = JobResult(title='t', company='c', location='l', arrangement='a', salary='s', date_posted='d', description='d', url='u', source='s'); assert j.salary_min is None"`
2. `python -c "from job_radar.sources import strip_html_and_normalize; assert strip_html_and_normalize('<b>test &amp; data</b>') == 'test & data'"`
3. `python -c "from job_radar.sources import parse_location_to_city_state; assert parse_location_to_city_state('Remote') == 'Remote'"`
4. `python -c "from job_radar.deduplication import deduplicate_cross_source; print('dedup module imports OK')"`
5. `python -m pytest tests/ -x -q` — All existing tests still pass
6. `pip show rapidfuzz` — Package installed
</verification>

<success_criteria>
- JobResult has 3 new optional fields (salary_min, salary_max, salary_currency) that default to None
- All existing scraper code works without modification (backward compatible)
- strip_html_and_normalize removes HTML, decodes entities, normalizes whitespace
- parse_location_to_city_state handles US states, Remote, international, edge cases
- deduplicate_cross_source uses rapidfuzz to detect fuzzy duplicates across sources
- rapidfuzz listed in pyproject.toml dependencies
- All existing tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/13-job-source-apis/13-01-SUMMARY.md`
</output>
