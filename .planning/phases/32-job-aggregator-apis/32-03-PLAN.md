---
phase: 32-job-aggregator-apis
plan: 03
type: execute
wave: 2
depends_on: ["32-01", "32-02"]
files_modified:
  - job_radar/sources.py
  - job_radar/deduplication.py
  - job_radar/search.py
  - job_radar/gui/worker_thread.py
  - job_radar/report.py
autonomous: true

must_haves:
  truths:
    - "Search queries include JSearch queries for first 4 profile titles with location mapping"
    - "Search queries include USAJobs queries with titles + location + optional federal filters"
    - "fetch_all runs native sources first, then APIs, then aggregators to ensure original source wins in dedup"
    - "Deduplication tracks multi-source matches and reports dedup stats"
    - "CLI and GUI display dedup statistics in progress output"
  artifacts:
    - path: "job_radar/sources.py"
      provides: "Updated build_search_queries with jsearch/usajobs entries, updated fetch_all with aggregator phase, updated run_query dispatch"
      contains: "AGGREGATOR_SOURCES"
    - path: "job_radar/deduplication.py"
      provides: "Multi-source tracking during dedup, dedup stats return value"
      contains: "sources_count"
  key_links:
    - from: "job_radar/sources.py:build_search_queries"
      to: "job_radar/sources.py:fetch_all"
      via: "Queries generated with jsearch/usajobs source types dispatched in run_query"
      pattern: "source.*jsearch"
    - from: "job_radar/sources.py:fetch_all"
      to: "job_radar/deduplication.py:deduplicate_cross_source"
      via: "Results passed to dedup after all phases complete, native sources processed first"
      pattern: "deduplicate_cross_source"
    - from: "job_radar/search.py"
      to: "job_radar/deduplication.py"
      via: "Dedup stats displayed in CLI output"
      pattern: "dedup.*stats"
---

<objective>
Wire JSearch and USAJobs into the search pipeline with proper source ordering and enhanced deduplication.

Purpose: Connect the fetch functions from Plan 01 into the query builder and fetch orchestrator, implement three-phase source ordering (scrapers -> APIs -> aggregators) to ensure native sources win in dedup, enhance deduplication to track multi-source matches and report statistics.

Output: Updated query builder, three-phase fetch_all, enhanced deduplication with stats, updated CLI/GUI progress output.
</objective>

<execution_context>
@/home/corye/.claude/get-shit-done/workflows/execute-plan.md
@/home/corye/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/32-job-aggregator-apis/32-RESEARCH.md
@.planning/phases/32-job-aggregator-apis/32-01-SUMMARY.md
@.planning/phases/32-job-aggregator-apis/32-02-SUMMARY.md
@job_radar/sources.py
@job_radar/deduplication.py
@job_radar/search.py
@job_radar/gui/worker_thread.py
@job_radar/report.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Update query builder and fetch_all for JSearch and USAJobs integration</name>
  <files>job_radar/sources.py</files>
  <action>
**Update build_search_queries():**

Add JSearch queries after the Authentic Jobs section (LOCKED DECISION: first 4 titles, same as Dice):
```python
# JSearch queries: each target title (aggregates LinkedIn, Indeed, Glassdoor)
for title in titles:
    jsearch_query = {"source": "jsearch", "query": title}
    # Location mapping (LOCKED DECISION): match location_preference
    arrangement = profile.get("arrangement", [])
    if "remote" in [a.lower() for a in arrangement]:
        jsearch_query["location"] = "remote"
    elif location:
        jsearch_query["location"] = location
    queries.append(jsearch_query)
```

Add USAJobs queries (LOCKED DECISION: titles + location, same pattern):
```python
# USAJobs queries: each target title
for title in titles:
    usajobs_query = {"source": "usajobs", "query": title}
    if location:
        usajobs_query["location"] = location
    queries.append(usajobs_query)
```

**Update run_query() dispatch in fetch_all:**
Add cases for new sources:
```python
elif q["source"] == "jsearch":
    return fetch_jsearch(q["query"], q.get("location", ""))
elif q["source"] == "usajobs":
    return fetch_usajobs(q["query"], q.get("location", ""), profile=profile)
```

Note: fetch_usajobs receives the full profile dict so it can extract federal filter fields (gs_grade_min, gs_grade_max, preferred_agencies).

**Update fetch_all source ordering (LOCKED DECISION: native source wins over aggregator):**

Change the two-phase system to three phases per research pitfall #4:
```python
SCRAPER_SOURCES = {"dice", "hn_hiring", "remoteok", "weworkremotely"}
API_SOURCES = {"adzuna", "authentic_jobs", "usajobs"}  # Add usajobs (native federal source)
AGGREGATOR_SOURCES = {"jsearch"}  # JSearch is aggregator — runs LAST
```

Update the query splitting:
```python
scraper_queries = [q for q in queries if q["source"] in SCRAPER_SOURCES]
api_queries = [q for q in queries if q["source"] in API_SOURCES]
aggregator_queries = [q for q in queries if q["source"] in AGGREGATOR_SOURCES]
```

Add Phase 3 after existing Phase 2:
```python
# Phase 3: Aggregators (run last — native sources win in dedup)
if aggregator_queries:
    log.debug("Phase 3: Running %d aggregator queries", len(aggregator_queries))
    aggregator_results = _run_queries_parallel(aggregator_queries, "aggregator")
    all_results.extend(aggregator_results)
```

**JSearch source splitting for progress display (LOCKED DECISION):**
JSearch is ONE API call but results should show individual source progress (LinkedIn: 5 jobs, Indeed: 7 jobs). This happens naturally because:
1. map_jsearch_to_job_result sets source to "linkedin"/"indeed"/"glassdoor"/"jsearch_other"
2. The source_job_counts tracking in _run_queries_parallel groups by q["source"]
3. BUT: JSearch queries use source="jsearch" not individual sources

To fix source splitting: After running JSearch queries, re-categorize results by their actual source field:
- In _run_queries_parallel, when processing JSearch results, update source_job_counts by the ACTUAL source on each JobResult, not the query source
- This means the progress callback will show the correct per-source counts

Implementation approach:
```python
# In _run_queries_parallel, after getting results from future:
for r in results:
    key = (r.title.lower().strip(), r.company.lower().strip())
    if key not in seen:
        seen.add(key)
        phase_results.append(r)
        # Track by actual source (for JSearch split display)
        actual_source = r.source  # "linkedin", "indeed", etc.
        source_job_counts[actual_source] = source_job_counts.get(actual_source, 0) + 1
```

This requires initializing source_job_counts for the JSearch display sources. Before running aggregator queries, initialize: `source_job_counts.setdefault("linkedin", 0)`, etc.

Also need to update total_sources count and source_names to include the individual display sources for JSearch, not "jsearch" itself:
```python
# After splitting queries, replace "jsearch" in source tracking with individual sources
JSEARCH_DISPLAY_SOURCES = ["linkedin", "indeed", "glassdoor"]
```

Compute total sources by replacing jsearch with its display sources in the source_names list.
  </action>
  <verify>
Run `python -c "from job_radar.sources import build_search_queries; profile = {'target_titles': ['Engineer'], 'target_market': 'Remote', 'core_skills': ['Python'], 'arrangement': ['remote']}; qs = build_search_queries(profile); sources = [q['source'] for q in qs]; assert 'jsearch' in sources; assert 'usajobs' in sources; print(f'Query sources: {set(sources)}')"` to verify query generation.
  </verify>
  <done>
build_search_queries generates jsearch and usajobs queries from profile titles. fetch_all uses three-phase ordering (scrapers -> APIs -> aggregators). run_query dispatches to fetch_jsearch and fetch_usajobs. JSearch results are split by actual source for progress display. Native sources run before aggregators ensuring dedup priority.
  </done>
</task>

<task type="auto">
  <name>Task 2: Enhance deduplication with multi-source tracking and stats</name>
  <files>job_radar/deduplication.py, job_radar/search.py, job_radar/gui/worker_thread.py, job_radar/report.py</files>
  <action>
**Enhance deduplication.py:**

Modify `deduplicate_cross_source()` to return a dict with results AND stats (LOCKED DECISION: show dedup stats + multi-source badge):

Change return type from `list` to `dict`:
```python
def deduplicate_cross_source(results: list, threshold: int = 85) -> dict:
    """Remove duplicate jobs across sources using fuzzy matching.

    Returns:
        dict with keys:
          - "results": Deduplicated list of JobResult objects
          - "stats": Dict with dedup statistics:
              - "original_count": total before dedup
              - "deduped_count": total after dedup
              - "duplicates_removed": number removed
              - "sources_involved": number of unique sources with duplicates
          - "multi_source": Dict mapping job key -> list of source names
              (only for jobs found on 2+ sources)
    """
```

Track multi-source badges during dedup:
```python
# When a fuzzy duplicate is found:
if is_duplicate:
    # Record this source for the multi-source badge
    seen_key = (seen_job.title.lower(), seen_job.company.lower())
    if seen_key not in multi_source_map:
        multi_source_map[seen_key] = [seen_job.source]
    if job.source not in multi_source_map[seen_key]:
        multi_source_map[seen_key].append(job.source)
```

Build stats at the end:
```python
sources_with_dupes = set()
for key, sources in multi_source_map.items():
    if len(sources) > 1:
        sources_with_dupes.update(sources)

stats = {
    "original_count": original_count,
    "deduped_count": deduped_count,
    "duplicates_removed": original_count - deduped_count,
    "sources_involved": len(sources_with_dupes),
}
```

Return `{"results": seen, "stats": stats, "multi_source": multi_source_map}`.

**IMPORTANT:** Since callers currently expect a list, ALL callers must be updated.

**Update sources.py fetch_all():**
Change the dedup call from:
```python
all_results = deduplicate_cross_source(all_results)
```
To:
```python
dedup_result = deduplicate_cross_source(all_results)
all_results = dedup_result["results"]
dedup_stats = dedup_result["stats"]
```

Update fetch_all to return both results and stats. Change return to:
```python
return all_results, dedup_stats
```

**Update search.py main() function:**
Where fetch_all is called, destructure the new return:
```python
all_results, dedup_stats = fetch_all(profile, on_progress=progress_callback)
```

Display dedup stats in CLI output (LOCKED DECISION: show in CLI progress AND report summary):
```python
if dedup_stats["duplicates_removed"] > 0:
    print(f"  {C.DIM}{dedup_stats['duplicates_removed']} duplicates removed across {dedup_stats['sources_involved']} sources{C.RESET}")
```

**Update gui/worker_thread.py:**
Where fetch_all is called, destructure:
```python
all_results, dedup_stats = fetch_all(self._profile, ...)
```
Pass dedup_stats to the completion message or store for report generation.

**Update sources_searched in both search.py and worker_thread.py:**
Add new sources to the sources_searched list:
```python
sources_searched = list({r["job"].source for r in scored}) if scored else [
    "Dice", "HN Hiring", "RemoteOK", "We Work Remotely",
    "Adzuna", "Authentic Jobs", "LinkedIn", "Indeed", "Glassdoor", "USAJobs (Federal)"
]
```

**Update report.py (if dedup stats display in report):**
If generate_report accepts dedup_stats parameter, add a small summary line in the report header showing "X duplicates removed across Y sources". Otherwise, the CLI/GUI output handles the stats display and no report changes needed. Only add report changes if the function signature naturally accommodates it — don't force an unneeded parameter.
  </action>
  <verify>
Run `python -c "from job_radar.deduplication import deduplicate_cross_source; result = deduplicate_cross_source([]); assert isinstance(result, dict); assert 'results' in result; assert 'stats' in result; print('dedup OK')"` to verify new return type. Run `python -m pytest tests/ -x -q` to check for regressions.
  </verify>
  <done>
deduplicate_cross_source returns dict with results, stats, and multi_source map. fetch_all returns (results, dedup_stats) tuple. search.py and worker_thread.py destructure new return value. CLI displays dedup stats. sources_searched includes all new source names. All callers updated to handle new return type.
  </done>
</task>

</tasks>

<verification>
1. build_search_queries includes jsearch and usajobs source queries
2. fetch_all has three-phase execution (scrapers -> APIs -> aggregators)
3. deduplicate_cross_source returns stats dict
4. CLI displays dedup statistics when duplicates found
5. sources_searched includes LinkedIn, Indeed, Glassdoor, USAJobs
6. Full test suite passes: `cd /home/corye/Claude/Job-Radar && python -m pytest tests/ -x -q`
</verification>

<success_criteria>
- JSearch and USAJobs queries generated from profile titles
- Three-phase source ordering ensures native source wins in dedup
- Dedup stats displayed in CLI output
- Multi-source tracking works (records which sources had same job)
- All tests pass including existing deduplication tests (updated for new return type)
</success_criteria>

<output>
After completion, create `.planning/phases/32-job-aggregator-apis/32-03-SUMMARY.md`
</output>
