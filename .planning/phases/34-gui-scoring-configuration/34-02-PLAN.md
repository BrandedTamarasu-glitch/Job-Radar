---
phase: 34-gui-scoring-configuration
plan: 02
type: execute
wave: 2
depends_on: ["34-01"]
files_modified:
  - job_radar/gui/main_window.py
  - tests/test_scoring_config.py
autonomous: false

must_haves:
  truths:
    - "Scoring Configuration section appears in Settings tab below API key settings"
    - "Users can expand/collapse the scoring section"
    - "Sliders show current profile weights on Settings tab load"
    - "Saving scoring config persists to profile.json"
    - "Tests verify widget creation, default values, normalize logic, and save validation"
  artifacts:
    - path: "job_radar/gui/main_window.py"
      provides: "Settings tab integration with ScoringConfigWidget"
      contains: "ScoringConfigWidget"
    - path: "tests/test_scoring_config.py"
      provides: "Unit tests for scoring config widget logic"
      min_lines: 80
  key_links:
    - from: "job_radar/gui/main_window.py"
      to: "job_radar/gui/scoring_config.py"
      via: "import and instantiation in _build_settings_tab"
      pattern: "from job_radar\\.gui\\.scoring_config import ScoringConfigWidget"
    - from: "job_radar/gui/main_window.py _build_settings_tab"
      to: "ScoringConfigWidget constructor"
      via: "widget creation with profile parameter"
      pattern: "ScoringConfigWidget\\("
---

<objective>
Integrate ScoringConfigWidget into the Settings tab and add unit tests for scoring config logic (normalization, validation, save pipeline).

Purpose: Connects the widget to the application and ensures correctness through automated tests.

Output: Updated `main_window.py` with scoring config in Settings tab, new `tests/test_scoring_config.py` with comprehensive tests.
</objective>

<execution_context>
@/Users/coryebert/.claude/get-shit-done/workflows/execute-plan.md
@/Users/coryebert/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/34-gui-scoring-configuration/34-CONTEXT.md
@.planning/phases/34-gui-scoring-configuration/34-RESEARCH.md
@.planning/phases/34-gui-scoring-configuration/34-01-SUMMARY.md (will exist from Plan 01)

Key references:
@job_radar/gui/main_window.py (_build_settings_tab method, line 773)
@job_radar/gui/scoring_config.py (ScoringConfigWidget from Plan 01)
@job_radar/profile_manager.py (DEFAULT_SCORING_WEIGHTS, save_profile, load_profile)
@tests/test_profile_manager.py (test patterns used in project)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Integrate ScoringConfigWidget into Settings tab</name>
  <files>job_radar/gui/main_window.py</files>
  <action>
Modify `_build_settings_tab` in `job_radar/gui/main_window.py` to add the ScoringConfigWidget below the existing API key settings.

**Import at top of file:**
```python
from job_radar.gui.scoring_config import ScoringConfigWidget
```

**In `_build_settings_tab` method, after the existing "Save API Keys" button (line ~871), add:**

1. A separator (visual break between API settings and scoring config):
   ```python
   separator = ctk.CTkFrame(scroll_frame, height=2, fg_color="gray70")
   separator.pack(fill="x", pady=(20, 10), padx=10)
   ```

2. Load the current profile to pass to the widget:
   ```python
   try:
       profile_path = get_data_dir() / "profile.json"
       profile = load_profile(profile_path)
   except Exception:
       profile = None
   ```

3. Create and pack the ScoringConfigWidget:
   ```python
   self._scoring_config = ScoringConfigWidget(
       scroll_frame,
       profile=profile,
       on_save_callback=self._on_scoring_saved
   )
   self._scoring_config.pack(fill="x", padx=10, pady=(10, 20))
   ```

4. Add `_on_scoring_saved` callback method to MainWindow:
   ```python
   def _on_scoring_saved(self):
       """Handle scoring configuration save success."""
       # No tab navigation needed -- user stays on Settings tab
       pass
   ```

**Important:** The ScoringConfigWidget should appear BELOW the API key save button but still INSIDE the scroll_frame, so it scrolls with the rest of the settings content. Do NOT create a separate tab.

**Do NOT modify any existing Settings tab functionality** -- the API key sections, test buttons, save button should remain exactly as they are. The scoring config is purely additive.
  </action>
  <verify>
Run `python -c "from job_radar.gui.main_window import MainWindow; print('Import OK')"` to verify the import chain works (main_window imports scoring_config).
  </verify>
  <done>
Settings tab shows ScoringConfigWidget below API key settings, separated by a visual divider. Widget loads current profile weights on tab build. No existing Settings tab functionality is modified.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add unit tests for scoring config logic</name>
  <files>tests/test_scoring_config.py</files>
  <action>
Create `tests/test_scoring_config.py` with tests that verify the scoring config widget's logic WITHOUT requiring a display server (no GUI rendering). Test the data logic, not the GUI rendering.

**Test approach:** Since GUI widget tests require a display server (which CI doesn't have), test the logic functions that can be extracted or called without rendering. Use unittest.mock to mock tkinter/customtkinter where needed.

**Tests to write:**

1. **test_sample_scores_has_all_components** - Verify SAMPLE_SCORES dict contains exactly the 6 scoring component keys matching DEFAULT_SCORING_WEIGHTS keys.

2. **test_default_weights_sum_to_one** - Verify DEFAULT_SCORING_WEIGHTS values sum to 1.0 (within 0.01 tolerance). This is a sanity check that the constant hasn't drifted.

3. **test_normalize_weights_proportional** - Test normalization logic: given weights {"skill_match": 0.5, "title_relevance": 0.3, "seniority": 0.3, "location": 0.3, "domain": 0.2, "response_likelihood": 0.4} (sum=2.0), normalization should produce values that sum to 1.0 while preserving relative ratios. Implement as a standalone function test:
   ```python
   def normalize_weights(weights):
       total = sum(weights.values())
       if total == 0:
           n = len(weights)
           return {k: round(1.0 / n, 2) for k in weights}
       return {k: round(v / total, 2) for k in weights}
   ```
   Import this from scoring_config.py (refactor the normalize logic into a module-level function if it isn't already).

4. **test_normalize_weights_all_zeros** - Test normalization with all-zero weights produces equal distribution (1/6 each).

5. **test_preview_calculation** - Test that the preview score calculation (weights * sample_scores, then sum) produces expected results with DEFAULT_SCORING_WEIGHTS. Expected: 4.5*0.25 + 4.0*0.15 + 3.8*0.15 + 5.0*0.15 + 3.5*0.10 + 3.2*0.20 = 1.125 + 0.6 + 0.57 + 0.75 + 0.35 + 0.64 = 4.035.

6. **test_preview_with_staffing_boost** - Test that staffing boost adds 0.5 to score (capped at 5.0).

7. **test_preview_with_staffing_penalize** - Test that staffing penalize subtracts 1.0 from score (floored at 1.0).

8. **test_staffing_display_mapping** - Test that the display-to-internal mapping covers all 3 values: "boost", "neutral", "penalize".

9. **test_weight_validation_valid** - Test that weights summing to 1.0 (within tolerance) pass validation.

10. **test_weight_validation_invalid_sum** - Test that weights summing to 0.8 fail validation.

11. **test_weight_validation_below_minimum** - Test that any weight below 0.05 fails validation.

**Implementation note:** To make normalize_weights and validate_weights testable without GUI, expose them as module-level functions in scoring_config.py. If Task 1 embedded them as methods, refactor them to be standalone functions that the class methods call. This is a minor refactor within scoring_config.py (also listed in files_modified for Plan 01, but Plan 01 is complete by now so this is a safe additive change).

If normalize_weights and validate_weights are already class methods, either:
- Extract them as module-level functions (preferred for testability)
- Or test them via the class with mocked tkinter

**Test file structure:**
```python
import pytest
from job_radar.gui.scoring_config import (
    normalize_weights, validate_weights, SAMPLE_SCORES,
    STAFFING_DISPLAY_MAP, STAFFING_INTERNAL_MAP
)
from job_radar.profile_manager import DEFAULT_SCORING_WEIGHTS
```
  </action>
  <verify>
Run `pytest tests/test_scoring_config.py -v` to verify all tests pass. If pytest is not available, run `python -m pytest tests/test_scoring_config.py -v`.
  </verify>
  <done>
11 tests in test_scoring_config.py covering: sample scores completeness, default weights sanity, normalization (proportional + zeros), preview calculation (default + boost + penalize), staffing mapping, and weight validation (valid sum, invalid sum, below minimum). All tests pass with zero regressions.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
Complete scoring configuration GUI integrated into the Settings tab:
- 6 weight sliders organized in "Skills & Fit" and "Context" groups
- Staffing firm preference dropdown (Boost/Neutral/Penalize)
- Live score preview panel showing sample job breakdown
- Normalize, Reset, and Save buttons
- Inline validation warning for invalid weight sums
- Collapsible section header
  </what-built>
  <how-to-verify>
1. Launch the GUI: `python -m job_radar` (or run the GUI entry point)
2. Navigate to the Settings tab
3. Scroll down past API key settings -- you should see a "Scoring Configuration" section
4. **Sliders:** Verify 6 sliders are visible with labels and percentage values. Drag any slider and confirm the value label updates immediately.
5. **Live Preview:** Confirm the right panel shows a "Score Preview" with component breakdown. Moving sliders should change the numbers in real-time.
6. **Normalize:** Set several sliders to random values (sum won't be 1.0). Confirm orange warning text appears. Click "Normalize" and verify all values adjust and warning disappears.
7. **Staffing Dropdown:** Change from Neutral to Boost -- preview should show "+0.5 staffing boost" line. Change to Penalize -- should show "-1.0 staffing penalty".
8. **Reset:** Click "Reset to Defaults" -- confirm dialog appears. Click OK. Verify all sliders return to default positions and dropdown returns to Neutral.
9. **Save:** Adjust some weights, normalize, then click Save. Confirm "Saved!" feedback appears. Close and reopen app -- verify the saved weights persist.
10. **Collapse:** Click the "Scoring Configuration" header to collapse, then expand again.
  </how-to-verify>
  <resume-signal>Type "approved" or describe any issues you see</resume-signal>
</task>

</tasks>

<verification>
1. `python -c "from job_radar.gui.main_window import MainWindow; print('OK')"` -- full import chain works
2. `python -c "from job_radar.gui.scoring_config import ScoringConfigWidget, normalize_weights, validate_weights, SAMPLE_SCORES; print('OK')"` -- all exports available
3. `pytest tests/test_scoring_config.py -v` -- all tests pass
4. `pytest tests/ -x` -- full test suite passes (zero regressions)
5. `grep "ScoringConfigWidget" job_radar/gui/main_window.py` -- confirms integration
6. `grep "scoring_config" job_radar/gui/main_window.py` -- confirms import
</verification>

<success_criteria>
- ScoringConfigWidget appears in Settings tab below API keys
- Widget loads current profile weights on display
- All 11 tests pass
- Full test suite passes with zero regressions
- Visual verification confirms: sliders work, preview updates, normalize/reset/save function correctly
</success_criteria>

<output>
After completion, create `.planning/phases/34-gui-scoring-configuration/34-02-SUMMARY.md`
</output>
