---
phase: 04-config-module-unit-tests
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/test_config.py
autonomous: true

must_haves:
  truths:
    - "load_config() with missing file returns {} without errors"
    - "load_config() with invalid JSON warns to stderr and returns {}"
    - "load_config() with unknown keys warns to stderr naming each key and filters them out"
    - "load_config() with valid keys returns only recognized keys"
    - "load_config() with non-dict JSON warns to stderr and returns {}"
    - "DEFAULT_CONFIG_PATH expands ~ to user home directory"
    - "KNOWN_KEYS contains exactly min_score, new_only, output"
  artifacts:
    - path: "tests/test_config.py"
      provides: "Parametrized config module tests"
      min_lines: 80
  key_links:
    - from: "tests/test_config.py"
      to: "job_radar/config.py"
      via: "import load_config, DEFAULT_CONFIG_PATH, KNOWN_KEYS"
      pattern: "from job_radar\\.config import"
---

<objective>
Create comprehensive parametrized unit tests for the config module covering all load_config() edge cases, DEFAULT_CONFIG_PATH tilde expansion, and KNOWN_KEYS validation.

Purpose: Close tech debt gap -- config.py is the only module added in Phases 1-3 that lacks dedicated unit tests. These tests provide regression protection for config file loading behavior.
Output: tests/test_config.py with all tests passing via `pytest tests/test_config.py -v`
</objective>

<execution_context>
@/Users/coryebert/.claude/get-shit-done/workflows/execute-plan.md
@/Users/coryebert/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-config-module-unit-tests/04-RESEARCH.md

@job_radar/config.py
@tests/conftest.py
@tests/test_tracker.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create tests/test_config.py with parametrized tests for all load_config edge cases</name>
  <files>tests/test_config.py</files>
  <action>
Create tests/test_config.py following established patterns from test_tracker.py and test_scoring.py. Import load_config, DEFAULT_CONFIG_PATH, and KNOWN_KEYS from job_radar.config.

Organize tests into these sections (use section comment headers matching existing test file style):

**Section 1: load_config() missing file (Success Criteria 1)**
- Test: pass a tmp_path path to nonexistent file, assert returns {}

**Section 2: load_config() invalid JSON (Success Criteria 2)**
- Parametrize with ids for multiple invalid inputs:
  - `"malformed_json"`: content `'{"invalid": json'` -> returns {}, stderr contains "Warning: Could not parse config file"
  - `"empty_file"`: content `''` -> returns {}, stderr contains warning
- Use capsys to verify stderr warning contains both "Warning" and the file path string

**Section 3: load_config() non-dict JSON (Success Criteria 2)**
- Parametrize with ids for non-dict types:
  - `"json_array"`: content `'[1, 2, 3]'` -> returns {}, stderr contains "must be a JSON object"
  - `"json_string"`: content `'"hello"'` -> returns {}, stderr contains "must be a JSON object"
  - `"json_number"`: content `'42'` -> returns {}, stderr contains "must be a JSON object"
  - `"json_null"`: content `'null'` -> returns {}, stderr contains "must be a JSON object"
- Each case: assert result == {} and capsys stderr contains "must be a JSON object"

**Section 4: load_config() unknown keys (Success Criteria 3)**
- Test single unknown key: content `'{"unknown_key": "value"}'` -> returns {}, stderr contains `"Unrecognized config key: 'unknown_key'"`
- Test multiple unknown keys: content `'{"bad1": 1, "bad2": 2}'` -> returns {}, stderr contains warning for each key
- Test mixed valid+invalid: content `'{"min_score": 3.0, "bad_key": "x"}'` -> returns `{"min_score": 3.0}`, stderr contains warning for "bad_key"

**Section 5: load_config() valid configs (Success Criteria 3)**
- Parametrize with ids:
  - `"single_key"`: content `'{"min_score": 3.5}'` -> `{"min_score": 3.5}`
  - `"all_keys"`: content `'{"min_score": 3.0, "new_only": true, "output": "/tmp/out"}'` -> `{"min_score": 3.0, "new_only": True, "output": "/tmp/out"}`
  - `"empty_object"`: content `'{}'` -> `{}`

**Section 6: DEFAULT_CONFIG_PATH tilde expansion (Success Criteria 4)**
- Test that `str(DEFAULT_CONFIG_PATH)` starts with `"~"`
- Test that `DEFAULT_CONFIG_PATH.expanduser()` starts with `str(Path.home())`
- Test that `"~"` is NOT in `str(DEFAULT_CONFIG_PATH.expanduser())`
- Test that expanded path ends with `.job-radar/config.json`

**Section 7: KNOWN_KEYS validation (Success Criteria 5)**
- Parametrize with ids:
  - `"min_score_valid"`: `"min_score"` in KNOWN_KEYS is True
  - `"new_only_valid"`: `"new_only"` in KNOWN_KEYS is True
  - `"output_valid"`: `"output"` in KNOWN_KEYS is True
  - `"profile_rejected"`: `"profile"` in KNOWN_KEYS is False
  - `"config_rejected"`: `"config"` in KNOWN_KEYS is False
  - `"unknown_rejected"`: `"unknown"` in KNOWN_KEYS is False
- Also verify KNOWN_KEYS has exactly 3 members: `assert len(KNOWN_KEYS) == 3`

All tests use tmp_path for file-based tests and capsys for stderr assertions. Use descriptive docstrings. Follow the comment header style from test_tracker.py (dashed lines with section names).
  </action>
  <verify>
Run `pytest tests/test_config.py -v` from the project root -- all tests must pass with zero failures.
Then run `pytest tests/ -v` to confirm no regressions in existing test_scoring.py and test_tracker.py.
  </verify>
  <done>
tests/test_config.py exists with parametrized tests covering all 6 success criteria from the phase definition. All tests pass. No regressions in existing tests.
  </done>
</task>

</tasks>

<verification>
1. `pytest tests/test_config.py -v` -- all tests pass, zero failures
2. `pytest tests/ -v` -- all tests across all modules pass (no regressions)
3. tests/test_config.py imports from job_radar.config (load_config, DEFAULT_CONFIG_PATH, KNOWN_KEYS)
4. All file-based tests use tmp_path (no hardcoded paths)
5. All stderr tests use capsys (not manual redirect)
6. Every success criterion from the phase definition maps to at least one test
</verification>

<success_criteria>
- tests/test_config.py exists with 80+ lines of parametrized tests
- `pytest tests/test_config.py -v` passes all tests
- `pytest tests/ -v` passes all tests (zero regressions)
- Missing file, invalid JSON, non-dict JSON, unknown keys, valid configs, DEFAULT_CONFIG_PATH, and KNOWN_KEYS all have dedicated test coverage
</success_criteria>

<output>
After completion, create `.planning/phases/04-config-module-unit-tests/04-01-SUMMARY.md`
</output>
