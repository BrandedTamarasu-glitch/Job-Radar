---
phase: 12-api-foundation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - job_radar/api_config.py
  - job_radar/rate_limits.py
  - .env.example
  - .gitignore
  - pyproject.toml
autonomous: true

must_haves:
  truths:
    - "API keys are loaded from .env file using python-dotenv (never hardcoded in source)"
    - "Missing API keys log a warning and return None (skip source, don't crash)"
    - "Rate limiting infrastructure uses per-source SQLite-backed persistent state"
    - "Rate limit state persists across application restarts"
    - ".env.example template documents required keys with signup URLs for Adzuna and Authentic Jobs"
    - ".env and .rate_limits/ are gitignored to prevent credential or state leaks"
  artifacts:
    - path: "job_radar/api_config.py"
      provides: "Credential loading from .env, get_api_key() helper, ensure_env_example()"
      contains: "load_api_credentials"
    - path: "job_radar/rate_limits.py"
      provides: "Per-source rate limiter with SQLiteBucket persistence, check_rate_limit(), get_rate_limit_status()"
      contains: "check_rate_limit"
    - path: ".env.example"
      provides: "Template with ADZUNA_APP_ID, ADZUNA_APP_KEY, AUTHENTIC_JOBS_API_KEY and signup URLs"
      contains: "ADZUNA_APP_ID"
    - path: "pyproject.toml"
      provides: "python-dotenv and pyrate-limiter added to dependencies"
      contains: "python-dotenv"
  key_links:
    - from: "job_radar/api_config.py"
      to: ".env file"
      via: "python-dotenv load_dotenv(find_dotenv(usecwd=True))"
      pattern: "load_dotenv"
    - from: "job_radar/rate_limits.py"
      to: ".rate_limits/*.db"
      via: "pyrate-limiter SQLiteBucket"
      pattern: "SQLiteBucket"
---

<objective>
Create the credential management and rate limiting infrastructure modules that Phase 13 API integrations will use.

Purpose: Establish secure, persistent foundation for API key storage and rate limiting so that Adzuna and Authentic Jobs integrations (Phase 13) can be added without hardcoding credentials or hitting 429 errors.

Output: Two new modules (api_config.py, rate_limits.py), .env.example template, updated dependencies and .gitignore.
</objective>

<execution_context>
@/Users/coryebert/.claude/get-shit-done/workflows/execute-plan.md
@/Users/coryebert/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/12-api-foundation/12-RESEARCH.md
@.planning/phases/12-api-foundation/12-CONTEXT.md
@job_radar/tracker.py (reference for persist-to-disk pattern)
@job_radar/search.py (reference for logging patterns)
@pyproject.toml (add dependencies here)
@.gitignore (add .env and .rate_limits/ here)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create api_config.py and .env.example</name>
  <files>job_radar/api_config.py, .env.example, .gitignore, pyproject.toml</files>
  <action>
Create `job_radar/api_config.py` with three functions:

1. `load_api_credentials()`:
   - Uses `find_dotenv(usecwd=True)` to locate .env file in project root
   - If no .env found: `log.info("No .env file found - API sources will be skipped")` and return (not an error)
   - Calls `load_dotenv(dotenv_path, override=False)` to load without overwriting existing env vars
   - On syntax error: print clear error to stderr with path and "Run 'job-radar --setup-apis' to recreate", then `sys.exit(1)` (fail-fast per user decision)
   - Log debug message with loaded path on success

2. `get_api_key(key_name: str, source_name: str) -> str | None`:
   - Gets key from `os.getenv(key_name)`
   - If missing: `log.warning(f"Skipping {source_name}: {key_name} not found in .env file. Run 'job-radar --setup-apis' to configure.")` then return None
   - If present: return the key value
   - One warning per source per run (log.warning is fine since it only fires once per fetch cycle)

3. `ensure_env_example()`:
   - Checks if `.env.example` exists in cwd, returns early if so
   - Creates `.env.example` with template content (see below)
   - Handles OSError gracefully (log warning, don't crash)

Create `.env.example` in project root with:
```
# Job Radar API Configuration
# Copy this file to .env and fill in your API keys

# Adzuna API Credentials
# Sign up at: https://developer.adzuna.com/
ADZUNA_APP_ID=
ADZUNA_APP_KEY=

# Authentic Jobs API Key
# Sign up at: https://authenticjobs.com/api/
AUTHENTIC_JOBS_API_KEY=
```

Update `.gitignore` — add these lines:
```
.env
.rate_limits/
```

Update `pyproject.toml` dependencies — add `python-dotenv` and `pyrate-limiter` to the dependencies list:
```
dependencies = ["requests", "beautifulsoup4", "platformdirs>=4.0", "pyfiglet", "colorama", "certifi", "questionary", "python-dotenv", "pyrate-limiter"]
```

Use the same logging pattern as other modules: `log = logging.getLogger(__name__)`.
Import from `dotenv` package (installed as `python-dotenv` but imported as `dotenv`).
Follow the existing code style: module docstring, stdlib imports first, third-party second, type hints on function signatures.
  </action>
  <verify>
Run: `python -c "from job_radar.api_config import load_api_credentials, get_api_key, ensure_env_example; print('imports OK')"`
Verify .env.example exists and contains ADZUNA_APP_ID, ADZUNA_APP_KEY, AUTHENTIC_JOBS_API_KEY.
Verify .gitignore contains `.env` and `.rate_limits/`.
Verify pyproject.toml contains `python-dotenv` and `pyrate-limiter` in dependencies.
  </verify>
  <done>
api_config.py imports cleanly. .env.example template documents all API keys with signup URLs. .env and .rate_limits/ are gitignored. Dependencies declared in pyproject.toml.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create rate_limits.py with persistent SQLite backend</name>
  <files>job_radar/rate_limits.py</files>
  <action>
Create `job_radar/rate_limits.py` with:

1. Module-level `RATE_LIMITS` dict mapping source names to rate configurations:
   - `"adzuna"`: `[Rate(100, Duration.MINUTE), Rate(1000, Duration.HOUR)]` (conservative estimate, no official docs)
   - `"authentic_jobs"`: `[Rate(60, Duration.MINUTE)]` (conservative default, no official docs)
   - Default fallback: `Rate(60, Duration.MINUTE)` for any unlisted source

2. Module-level `_limiters: dict[str, Limiter] = {}` cache to avoid re-creating limiter objects.

3. `get_rate_limiter(source: str) -> Limiter`:
   - Return cached limiter from `_limiters` if exists
   - Create `.rate_limits/` directory in cwd (like `.cache/` pattern from cache.py)
   - Create SQLite file at `.rate_limits/{source}.db`
   - Get rates from `RATE_LIMITS.get(source, [Rate(60, Duration.MINUTE)])`
   - Create `SQLiteBucket(rates, str(db_path))` for persistent state
   - Create `Limiter(bucket)`, cache in `_limiters`, return it
   - Log debug message on initialization

4. `check_rate_limit(source: str, verbose: bool = False) -> bool`:
   - Get limiter via `get_rate_limiter(source)`
   - If verbose: log remaining calls and reset time via bucket state inspection
   - Call `limiter.try_acquire(source)` with non-blocking behavior (per user decision: skip immediately, don't wait)
   - If rate limited: compute retry time and log warning. To extract the reset timestamp:
     a. pyrate-limiter's `try_acquire()` raises `BucketFullException` when rate limited (rather than returning False). Catch `BucketFullException` — it has a `meta_info` dict with key `"remaining_time"` (float, seconds until bucket has capacity).
     b. If `try_acquire` returns a bool instead (API may vary by version), fall back to logging without retry time.
     c. Compute reset datetime: `reset_at = datetime.datetime.now() + datetime.timedelta(seconds=remaining_time)`
     d. Format: `reset_at.strftime('%I:%M%p').lstrip('0').lower()` to produce e.g. "2:35pm" (strip leading zero from hour, lowercase am/pm)
     e. Log: `log.warning(f"Skipped {source}: rate limited, retry after {reset_str}")`
     f. IMPORTANT: The executor MUST inspect the installed pyrate-limiter v3.14.0 API at runtime (e.g., `python -c "from pyrate_limiter import BucketFullException; help(BucketFullException)"`) to confirm the exception structure. If `meta_info` or `remaining_time` is not available, use `time.time()` + the shortest configured rate's duration as a rough estimate.
   - Return True if allowed, False if rate limited

5. `get_rate_limit_status(source: str) -> dict`:
   - Returns dict with `remaining`, `reset_time`, `configured_rate` for debugging
   - To get remaining capacity: call `bucket.count()` or inspect bucket internals to determine how many tokens remain in the current window. If no direct "remaining" method exists, compute it from `configured_rate - calls_made_in_window`.
   - For reset_time: use the same `remaining_time` from `BucketFullException` approach (attempt a dry acquire, catch exception, extract time), or estimate from configured rate duration + last call timestamp.
   - Handles first-call case (no bucket state yet) gracefully — return `{"remaining": "unknown", "reset_time": None, "configured_rate": ...}`

IMPORTANT — pyrate-limiter API verification strategy: The research shows `SQLiteBucket`, `Limiter`, `Rate`, `Duration`, `BucketFullException` as the key classes. The executor MUST verify the actual installed API before writing code:
   a. Run `python -c "import pyrate_limiter; print(dir(pyrate_limiter))"` to see available exports
   b. Run `python -c "from pyrate_limiter import Limiter; help(Limiter.try_acquire)"` to check method signature and return type
   c. Run `python -c "from pyrate_limiter import BucketFullException; help(BucketFullException)"` to check exception attributes
   d. If `try_acquire` on Limiter doesn't exist, use `bucket.try_acquire()` directly
   e. The key behaviors are: non-blocking check, persist to SQLite, extract remaining_time for retry calculation
   f. If the API is significantly different from what's described, adapt accordingly — the user decision that must be honored is: "Rate limit warnings include retry time: 'Skipped Adzuna: rate limited, retry after 2:35pm'" with `%I:%M%p`-style formatting (lowercase, no leading zero on hour).

Use same logging pattern: `log = logging.getLogger(__name__)`.
Follow existing code style with module docstring.
  </action>
  <verify>
Run: `pip install python-dotenv pyrate-limiter` (in project venv)
Run: `python -c "from job_radar.rate_limits import check_rate_limit, get_rate_limit_status, RATE_LIMITS; print('imports OK'); print(f'Sources: {list(RATE_LIMITS.keys())}')"`
Verify .rate_limits/ directory can be created and SQLite files written.
  </verify>
  <done>
rate_limits.py imports cleanly. RATE_LIMITS dict has entries for adzuna and authentic_jobs. check_rate_limit returns bool. SQLite files persist to .rate_limits/ directory. Rate limit state survives across Python process restarts.
  </done>
</task>

</tasks>

<verification>
1. `python -c "from job_radar.api_config import load_api_credentials, get_api_key"` succeeds
2. `python -c "from job_radar.rate_limits import check_rate_limit, get_rate_limit_status"` succeeds
3. `.env.example` exists with ADZUNA_APP_ID, ADZUNA_APP_KEY, AUTHENTIC_JOBS_API_KEY
4. `.gitignore` includes `.env` and `.rate_limits/`
5. `pyproject.toml` lists `python-dotenv` and `pyrate-limiter` as dependencies
6. `check_rate_limit("adzuna")` returns True on fresh state (no prior calls)
</verification>

<success_criteria>
- api_config.py provides load_api_credentials() and get_api_key() with graceful degradation
- rate_limits.py provides check_rate_limit() with SQLite persistence per source
- .env.example documents all required API keys with signup URLs
- .env and .rate_limits/ are gitignored
- python-dotenv and pyrate-limiter added to pyproject.toml dependencies
- All imports succeed without errors
</success_criteria>

<output>
After completion, create `.planning/phases/12-api-foundation/12-01-SUMMARY.md`
</output>
