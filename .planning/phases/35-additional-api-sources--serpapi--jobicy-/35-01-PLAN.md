---
phase: 35-additional-api-sources--serpapi--jobicy-
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - job_radar/sources.py
  - job_radar/rate_limits.py
autonomous: true

must_haves:
  truths:
    - "SerpAPI Google Jobs fetch returns JobResult list with correct source attribution"
    - "Jobicy remote jobs fetch returns JobResult list with HTML descriptions cleaned"
    - "Rate limiters for serpapi and jobicy are configured with conservative defaults"
    - "Search pipeline includes SerpAPI and Jobicy queries in aggregator phase"
    - "Quota usage can be queried for any rate-limited source"
  artifacts:
    - path: "job_radar/sources.py"
      provides: "fetch_serpapi(), fetch_jobicy(), map_serpapi_to_job_result(), map_jobicy_to_job_result()"
      contains: "def fetch_serpapi"
    - path: "job_radar/rate_limits.py"
      provides: "serpapi/jobicy rate configs, BACKEND_API_MAP entries, get_quota_usage()"
      contains: "def get_quota_usage"
  key_links:
    - from: "job_radar/sources.py"
      to: "job_radar/rate_limits.py"
      via: "check_rate_limit('serpapi') and check_rate_limit('jobicy')"
      pattern: "check_rate_limit.*serpapi|check_rate_limit.*jobicy"
    - from: "job_radar/sources.py:build_search_queries"
      to: "job_radar/sources.py:fetch_serpapi"
      via: "serpapi query entries in build_search_queries"
      pattern: "source.*serpapi"
    - from: "job_radar/sources.py:fetch_all"
      to: "job_radar/sources.py:fetch_serpapi"
      via: "run_query dispatches to fetch_serpapi/fetch_jobicy"
      pattern: "fetch_serpapi|fetch_jobicy"
---

<objective>
Implement SerpAPI Google Jobs and Jobicy remote jobs backend integration: fetch functions, response mappers, rate limiter configuration, and search pipeline integration.

Purpose: Provide the core data retrieval layer for two new job sources so users get broader job coverage from Google Jobs aggregation and remote-focused Jobicy listings.
Output: Working fetch_serpapi() and fetch_jobicy() functions in sources.py, rate limiter configs in rate_limits.py, get_quota_usage() utility, and full search pipeline integration.
</objective>

<execution_context>
@/Users/coryebert/.claude/get-shit-done/workflows/execute-plan.md
@/Users/coryebert/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Key reference files - read these before implementing
@job_radar/sources.py  # Existing fetch_jsearch (line 959), fetch_usajobs (line 1106), map_jsearch_to_job_result (line 1018), build_search_queries (line 1451), fetch_all (line 1529), _SOURCE_DISPLAY_NAMES (line 587), strip_html_and_normalize (line 71)
@job_radar/rate_limits.py  # RATE_LIMITS defaults (line 56), BACKEND_API_MAP (line 110), _connections (line 124), get_rate_limiter (line 171)
@job_radar/api_config.py  # get_api_key() pattern

# Phase context
@.planning/phases/35-additional-api-sources--serpapi--jobicy-/35-CONTEXT.md
@.planning/phases/35-additional-api-sources--serpapi--jobicy-/35-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add SerpAPI and Jobicy rate limiter config and quota tracking utility</name>
  <files>job_radar/rate_limits.py</files>
  <action>
  Make these changes to job_radar/rate_limits.py:

  1. **Add rate limit defaults** in `_load_rate_limits()` defaults dict (around line 56):
     ```python
     "serpapi": [Rate(50, Duration.MINUTE)],  # Conservative for free tier (100 searches/month cap)
     "jobicy": [Rate(1, Duration.HOUR)],      # Per docs: "once per hour"
     ```

  2. **Add BACKEND_API_MAP entries** (around line 110):
     ```python
     # SerpAPI — single source
     "serpapi": "serpapi",
     # Jobicy — single source (no API key required, but rate limited)
     "jobicy": "jobicy",
     ```

  3. **Add get_quota_usage() function** after `get_rate_limit_status()` (after line 312). This queries the pyrate-limiter SQLite bucket tables directly to calculate usage within the current rate window:
     ```python
     def get_quota_usage(source: str) -> tuple[int, int, str] | None:
         """Get current quota usage for a rate-limited source.

         Queries the pyrate-limiter SQLite bucket table directly to count
         items within the current rate window. Returns (used, limit, period)
         tuple or None if quota info is not available.

         Parameters
         ----------
         source : str
             Source name (e.g., "serpapi", "jsearch", "adzuna")

         Returns
         -------
         tuple[int, int, str] | None
             (used_count, limit, period_label) or None if not available.
             period_label is one of "minute", "hour", "day", or "{N}s".
         """
         import time

         backend_api = BACKEND_API_MAP.get(source, source)

         # Get rate configuration
         rates = RATE_LIMITS.get(backend_api)
         if not rates:
             return None

         # Get SQLite connection (must already be initialized)
         conn = _connections.get(backend_api)
         if not conn:
             return None

         # Use the shortest rate window for display (most relevant for quota)
         shortest_rate = min(rates, key=lambda r: r.interval)
         limit = shortest_rate.limit
         interval_seconds = shortest_rate.interval

         # Calculate current window start
         now = time.time()
         window_start = now - interval_seconds

         try:
             cursor = conn.execute(
                 "SELECT COUNT(*) FROM rate_limits WHERE created_at >= ?",
                 (window_start,)
             )
             used = cursor.fetchone()[0]

             # Format period label
             if interval_seconds <= 60:
                 period = "minute"
             elif interval_seconds <= 3600:
                 period = "hour"
             elif interval_seconds <= 86400:
                 period = "day"
             else:
                 period = f"{int(interval_seconds)}s"

             return (used, limit, period)
         except Exception as e:
             log.debug("Could not get quota for %s: %s", source, e)
             return None
     ```

  Note: The SQLite queries use read-only SELECT with no modifications to avoid conflicts with pyrate-limiter's background threads. The function returns None gracefully if the connection hasn't been initialized (source not yet used) or if the query fails.
  </action>
  <verify>
  Run: `python -c "from job_radar.rate_limits import RATE_LIMITS, BACKEND_API_MAP, get_quota_usage; assert 'serpapi' in RATE_LIMITS; assert 'jobicy' in RATE_LIMITS; assert 'serpapi' in BACKEND_API_MAP; assert 'jobicy' in BACKEND_API_MAP; print('Rate limit config OK')"` — should print "Rate limit config OK"
  </verify>
  <done>RATE_LIMITS contains serpapi (50/min) and jobicy (1/hour) entries; BACKEND_API_MAP maps both sources; get_quota_usage() function exists and returns tuple or None.</done>
</task>

<task type="auto">
  <name>Task 2: Implement SerpAPI and Jobicy fetch functions with response mappers</name>
  <files>job_radar/sources.py</files>
  <action>
  Add the following to job_radar/sources.py:

  1. **Add _SOURCE_DISPLAY_NAMES entries** (around line 587):
     ```python
     "serpapi": "SerpAPI (Google Jobs)",
     "jobicy": "Jobicy (Remote)",
     ```

  2. **Add map_serpapi_to_job_result()** function (after map_usajobs_to_job_result, around line 1270). Follow the exact pattern of map_jsearch_to_job_result():
     ```python
     def map_serpapi_to_job_result(item: dict) -> JobResult | None:
         """Map SerpAPI Google Jobs response item to JobResult.

         Validates required fields (title, company_name, apply link) and returns
         None if any are missing. Uses detected_extensions for work arrangement
         and employment type.
         """
         title = item.get("title", "").strip()
         company = item.get("company_name", "").strip()

         # URL from apply_options (first link) or fall back to share_link
         apply_options = item.get("apply_options", [])
         url = ""
         if apply_options and isinstance(apply_options, list):
             url = apply_options[0].get("link", "").strip()
         if not url:
             url = item.get("share_link", "").strip()

         if not title or not company or not url:
             log.debug("[SerpAPI] Skipping job with missing required fields: title=%s, company=%s, url=%s",
                      bool(title), bool(company), bool(url))
             return None

         # Location
         location_raw = item.get("location", "")
         location = parse_location_to_city_state(location_raw)

         # Work arrangement from detected_extensions
         extensions = item.get("detected_extensions", {})
         if extensions.get("work_from_home"):
             arrangement = "remote"
         else:
             arrangement = _parse_arrangement(f"{title} {item.get('description', '')}")

         # Description
         description_raw = item.get("description", "")
         description = strip_html_and_normalize(description_raw)
         if len(description) > 500:
             description = description[:497] + "..."

         # Employment type
         emp_type = extensions.get("schedule_type", "")

         # Salary (SerpAPI rarely includes salary)
         salary = "Not specified"

         # Date posted
         date_posted = extensions.get("posted_at", "")

         return JobResult(
             title=_clean_field(title, _MAX_TITLE),
             company=_clean_field(company, _MAX_COMPANY),
             location=_clean_field(location, _MAX_LOCATION),
             arrangement=arrangement,
             salary=salary,
             date_posted=date_posted,
             description=description,
             url=url,
             source="serpapi",
             employment_type=emp_type,
             parse_confidence="high",
         )
     ```

  3. **Add map_jobicy_to_job_result()** function (after map_serpapi_to_job_result):
     ```python
     def map_jobicy_to_job_result(item: dict) -> JobResult | None:
         """Map Jobicy API response item to JobResult.

         Validates required fields (jobTitle, companyName, url). Cleans HTML
         from jobDescription using strip_html_and_normalize(). Jobicy jobs are
         always remote by definition.
         """
         title = item.get("jobTitle", "").strip()
         company = item.get("companyName", "").strip()
         url = item.get("url", "").strip()

         if not title or not company or not url:
             log.debug("[Jobicy] Skipping job with missing required fields: title=%s, company=%s, url=%s",
                      bool(title), bool(company), bool(url))
             return None

         # Location (Jobicy's jobGeo is region, not city/state)
         location_raw = item.get("jobGeo", "")
         location = location_raw if location_raw else "Remote"

         # All Jobicy jobs are remote by definition
         arrangement = "remote"

         # Description: HTML content, must be cleaned
         description_raw = item.get("jobDescription", "")
         if not description_raw:
             description_raw = item.get("jobExcerpt", "")
         description = strip_html_and_normalize(description_raw)
         if len(description) > 500:
             description = description[:497] + "..."

         # Skip jobs with empty description after cleaning (required for scoring)
         if not description:
             log.debug("[Jobicy] Skipping job with empty description: %s at %s", title, company)
             return None

         # Employment type
         emp_type = item.get("jobType", "")

         # Salary
         salary_min_str = item.get("annualSalaryMin", "")
         salary_max_str = item.get("annualSalaryMax", "")
         salary_currency = item.get("salaryCurrency", "USD")
         salary_min = None
         salary_max = None

         if salary_min_str and salary_max_str:
             try:
                 salary_min = float(salary_min_str)
                 salary_max = float(salary_max_str)
                 salary = f"{salary_currency} {int(salary_min):,}-{int(salary_max):,}/yr"
             except (ValueError, TypeError):
                 salary = "Not specified"
         elif salary_min_str:
             try:
                 salary_min = float(salary_min_str)
                 salary = f"{salary_currency} {int(salary_min):,}+/yr"
             except (ValueError, TypeError):
                 salary = "Not specified"
         else:
             salary = "Not specified"

         # Date posted
         date_posted = item.get("pubDate", "")

         return JobResult(
             title=_clean_field(title, _MAX_TITLE),
             company=_clean_field(company, _MAX_COMPANY),
             location=_clean_field(location, _MAX_LOCATION),
             arrangement=arrangement,
             salary=salary,
             date_posted=date_posted,
             description=description,
             url=url,
             source="jobicy",
             employment_type=emp_type,
             parse_confidence="high",
             salary_min=salary_min,
             salary_max=salary_max,
             salary_currency=salary_currency,
         )
     ```

  4. **Add fetch_serpapi()** function (after the mappers). Follow the exact pattern of fetch_jsearch() at line 959:
     ```python
     def fetch_serpapi(query: str, location: str = "", verbose: bool = False) -> list[JobResult]:
         """Fetch job listings from SerpAPI Google Jobs API."""
         results = []

         # Check credentials
         api_key = get_api_key("SERPAPI_API_KEY", "SerpAPI")
         if not api_key:
             return results

         # Check rate limit
         if not check_rate_limit("serpapi", verbose=verbose):
             return results

         # Build API URL
         params = {
             "engine": "google_jobs",
             "q": query,
             "api_key": api_key,
         }
         if location:
             params["location"] = location

         url = "https://serpapi.com/search?" + urllib.parse.urlencode(params)

         # Fetch with retry
         try:
             body = fetch_with_retry(url, headers=HEADERS, use_cache=True)
             if body is None:
                 log.debug("[SerpAPI] Fetch failed for '%s'", query)
                 return results

             data = _json.loads(body)
             items = data.get("jobs_results", [])

             for item in items:
                 job = map_serpapi_to_job_result(item)
                 if job:
                     results.append(job)

         except _json.JSONDecodeError as e:
             log.debug("[SerpAPI] JSON parse error: %s", e)
         except Exception as e:
             error_str = str(e).lower()
             if "401" in error_str or "403" in error_str or "unauthorized" in error_str:
                 log.error("[SerpAPI] Authentication failed - run 'job-radar --setup-apis' to reconfigure")
             else:
                 log.debug("[SerpAPI] Request failed: %s", e)

         log.info("[SerpAPI] Found %d results for '%s'", len(results), query)
         return results
     ```

  5. **Add fetch_jobicy()** function (after fetch_serpapi). Jobicy does NOT require an API key — it is a public API:
     ```python
     def fetch_jobicy(query: str, location: str = "", verbose: bool = False) -> list[JobResult]:
         """Fetch remote job listings from Jobicy API.

         Jobicy is a public API (no key required) but rate limited to 1 request/hour.
         Returns remote-focused job listings with HTML descriptions cleaned.
         """
         results = []

         # Check rate limit (no API key needed, but strict rate limit)
         if not check_rate_limit("jobicy", verbose=verbose):
             return results

         # Build API URL
         params = {"count": "20"}

         # Map location to Jobicy geo filter if applicable
         if location:
             location_lower = location.lower()
             if any(term in location_lower for term in ["usa", "united states", "us"]):
                 params["geo"] = "usa"
             elif any(term in location_lower for term in ["uk", "united kingdom", "britain"]):
                 params["geo"] = "uk"
             elif any(term in location_lower for term in ["canada"]):
                 params["geo"] = "canada"
             elif any(term in location_lower for term in ["europe"]):
                 params["geo"] = "europe"
             # For specific cities/states, skip geo filter (Jobicy uses broad regions)

         # Use query as tag parameter for filtering
         if query:
             params["tag"] = query.lower().replace(" ", "-")

         url = "https://jobicy.com/api/v2/remote-jobs?" + urllib.parse.urlencode(params)

         # Fetch with retry
         try:
             body = fetch_with_retry(url, headers=HEADERS, use_cache=True)
             if body is None:
                 log.debug("[Jobicy] Fetch failed for '%s'", query)
                 return results

             data = _json.loads(body)
             # Jobicy returns {"jobs": [...]} wrapper
             items = data.get("jobs", [])
             if not isinstance(items, list):
                 items = []

             for item in items:
                 job = map_jobicy_to_job_result(item)
                 if job:
                     results.append(job)

         except _json.JSONDecodeError as e:
             log.debug("[Jobicy] JSON parse error: %s", e)
         except Exception as e:
             log.debug("[Jobicy] Request failed: %s", e)

         log.info("[Jobicy] Found %d results for '%s'", len(results), query)
         return results
     ```

  6. **Update build_search_queries()** (around line 1524, after USAJobs queries). Add SerpAPI and Jobicy query generation:
     ```python
     # SerpAPI queries: each target title (alternative Google Jobs aggregator)
     for title in titles:
         serpapi_query = {"source": "serpapi", "query": title}
         if location:
             serpapi_query["location"] = location
         queries.append(serpapi_query)

     # Jobicy queries: top 2 target titles (remote-focused, limited rate)
     for title in titles[:2]:
         queries.append({
             "source": "jobicy",
             "query": title,
             "location": location,
         })
     ```

  7. **Update fetch_all()** source ordering sets (around line 1547-1553):
     - Add "serpapi" to AGGREGATOR_SOURCES: `AGGREGATOR_SOURCES = {"jsearch", "serpapi"}`
     - Add "jobicy" to API_SOURCES: `API_SOURCES = {"adzuna", "authentic_jobs", "usajobs", "jobicy"}`
     (Jobicy is a native remote source so goes with APIs, SerpAPI is a Google Jobs aggregator so goes with aggregators)

  8. **Update run_query()** function inside fetch_all() (around line 1590-1607). Add dispatch for new sources:
     ```python
     elif q["source"] == "serpapi":
         return fetch_serpapi(q["query"], q.get("location", ""))
     elif q["source"] == "jobicy":
         return fetch_jobicy(q["query"], q.get("location", ""))
     ```
  </action>
  <verify>
  Run: `python -c "from job_radar.sources import fetch_serpapi, fetch_jobicy, map_serpapi_to_job_result, map_jobicy_to_job_result, build_search_queries; print('Import OK')"` — should print "Import OK"

  Run: `python -c "
from job_radar.sources import map_serpapi_to_job_result, map_jobicy_to_job_result

# Test SerpAPI mapper
serp_item = {'title': 'Software Engineer', 'company_name': 'Google', 'location': 'Mountain View, CA', 'description': 'Build stuff', 'apply_options': [{'link': 'https://example.com/apply'}], 'detected_extensions': {'posted_at': '2 days ago', 'schedule_type': 'Full-time'}}
job = map_serpapi_to_job_result(serp_item)
assert job is not None
assert job.source == 'serpapi'
assert job.title == 'Software Engineer'
assert job.company == 'Google'
print('SerpAPI mapper OK')

# Test Jobicy mapper
jobicy_item = {'jobTitle': 'Python Developer', 'companyName': 'Remote Corp', 'url': 'https://jobicy.com/jobs/123', 'jobGeo': 'USA', 'jobDescription': '<p>Build <b>cool</b> things</p>', 'jobType': 'full-time', 'pubDate': '2026-02-13 10:00:00'}
job2 = map_jobicy_to_job_result(jobicy_item)
assert job2 is not None
assert job2.source == 'jobicy'
assert job2.arrangement == 'remote'
assert '<p>' not in job2.description  # HTML stripped
print('Jobicy mapper OK')

# Test required field validation
assert map_serpapi_to_job_result({'title': '', 'company_name': 'X'}) is None
assert map_jobicy_to_job_result({'jobTitle': '', 'companyName': 'X'}) is None
print('Validation OK')
"` — should print all three OK lines

  Run: `python -c "
from job_radar.sources import build_search_queries
profile = {'target_titles': ['Software Engineer'], 'core_skills': ['python'], 'location': 'Remote', 'target_market': 'Remote'}
queries = build_search_queries(profile)
sources = [q['source'] for q in queries]
assert 'serpapi' in sources, f'serpapi not in {sources}'
assert 'jobicy' in sources, f'jobicy not in {sources}'
print('Query generation OK')
"` — should print "Query generation OK"

  Run existing tests: `cd /Users/coryebert/Job-Radar && python -m pytest tests/ -x -q --tb=short` — all tests should pass (no regressions)
  </verify>
  <done>
  fetch_serpapi() and fetch_jobicy() functions work correctly with proper credential checks, rate limiting, error handling, and response mapping. map_serpapi_to_job_result() and map_jobicy_to_job_result() validate required fields and clean data. build_search_queries() generates SerpAPI and Jobicy queries. fetch_all() dispatches to new fetchers in correct phase ordering. All existing tests pass.
  </done>
</task>

</tasks>

<verification>
1. `python -c "from job_radar.rate_limits import RATE_LIMITS, BACKEND_API_MAP, get_quota_usage; print('imports OK')"` succeeds
2. `python -c "from job_radar.sources import fetch_serpapi, fetch_jobicy, build_search_queries; print('imports OK')"` succeeds
3. SerpAPI mapper handles all field combinations (with/without apply_options, detected_extensions)
4. Jobicy mapper strips HTML from descriptions, handles salary formatting, sets arrangement="remote"
5. Both mappers return None for jobs missing required fields (title, company, url)
6. build_search_queries() generates serpapi and jobicy queries from profile
7. fetch_all() routes serpapi to aggregator phase and jobicy to API phase
8. `python -m pytest tests/ -x -q --tb=short` — all existing tests pass (zero regressions)
</verification>

<success_criteria>
- SerpAPI and Jobicy fetch functions follow exact pattern of existing JSearch/USAJobs fetchers
- Rate limiters configured with conservative defaults (serpapi: 50/min, jobicy: 1/hour)
- get_quota_usage() returns (used, limit, period) tuples from SQLite bucket queries
- Search pipeline generates and dispatches queries for both new sources
- No regressions in existing test suite
</success_criteria>

<output>
After completion, create `.planning/phases/35-additional-api-sources--serpapi--jobicy-/35-01-SUMMARY.md`
</output>
